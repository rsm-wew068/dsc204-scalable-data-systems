{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12d7ed3",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Introduction to Ray Data \n",
    "\n",
    "In this problem, we introduce Ray Data, and how to work with Ray Datasets. We highly encourage you to first go over [documentation](https://docs.ray.io/en/latest/data/data.html) for Ray Data. \n",
    "\n",
    "The dataset we will use for this problems is the Electronics subset of the Amazon Reviews dataset. This dataset has been provided to you in parquet format at path ``~/public/pa2``In the first section, you will use the ``read_parquet`` method to read your parquet dataset into a  Ray.data.Dataset object. Ray Data uses Ray Tasks to read files in parallel. [This](https://docs.ray.io/en/latest/data/data-internals.html) is a useful resource to understand how data loading works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f945a2-01ab-478b-85a4-3f417050067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this command once to upgrade ray, then comment this out to avoid\n",
    "# reinstall package in the grader account during autograding.\n",
    "\n",
    "# !pip install \"ray[default]\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56201a",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:18,090\tINFO worker.py:2004 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ray_env/lib/python3.10/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960a094f58154826a3f936c2212d7cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parquet dataset sampling 0:   0%|          | 0.00/1.00 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:19,262\tINFO parquet_datasource.py:699 -- Estimated parquet encoding ratio is 1.683.\n",
      "2025-11-02 17:20:19,263\tINFO parquet_datasource.py:759 -- Estimated parquet reader batch size at 285570 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(Map(lowercase)->MapBatches(scale)->MapBatches(add_column)->MapBatches(preprocessor) pid=4221)\u001b[0m Token indices sequence length is longer than the specified maximum sequence length for this model (1278 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "ds = ray.data.read_parquet(\"pa2_data_100k.parquet\")\n",
    "\n",
    "# sort dataset to make sure determinstic results\n",
    "ds = ds.sort([\"asin\", \"unixReviewTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81dc43ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column          Type\n",
       "------          ----\n",
       "reviewTime      string\n",
       "reviewerName    string\n",
       "summary         string\n",
       "unixReviewTime  int64\n",
       "asin            string\n",
       "reviewText      string\n",
       "reviewerID      string\n",
       "verified        bool\n",
       "overall         double"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print out the schema of the dataset\n",
    "ds.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af4236",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Check `num_blocks`\n",
    "Go through the documentation listed at the top to understand what `num_blocks` is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1cdbf4",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:19,574\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_2_0\n",
      "2025-11-02 17:20:19,584\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_2_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:19,584\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_2_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> AllToAllOperator[Sort]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c7d1bdc175491a850a4beec57fd069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f949e4f875f4d6b9d358eff7cd1e349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadParquet->SplitBlocks(44) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b625c103e70a403d9e698c4bcaf9c9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Sort 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7ca34b1aa8434484403902e33c4b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 3:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b771a090cb354b0e803fd6cfebf79be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 4:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957bbf16be7c4a25a941b2cfa9fc9c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 5:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:19,626\tWARNING resource_manager.py:134 -- ⚠️  Ray's object store is configured to use only 15.0% of available memory (2.0GiB out of 13.4GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2025-11-02 17:20:20,942\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_2_0 execution finished in 1.36 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.materialize()\n",
    "ds.num_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f1aa5fc",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb93cf6566debe1a1e156e69232350aa",
     "grade": false,
     "grade_id": "cell-64a7fbbf81f9b9ee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:21,044\tINFO dataset.py:3500 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2025-11-02 17:20:21,051\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_4_0\n",
      "2025-11-02 17:20:21,054\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_4_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:21,055\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_4_0: InputDataBuffer[Input] -> LimitOperator[limit=5]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01c8fe9fe85474886d45fa24e12e405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba4971d24ae46119f7f66838c4c1780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=5 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:21,128\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_4_0 execution finished in 0.07 seconds\n"
     ]
    }
   ],
   "source": [
    "# View the first 5 entries using the Dataset.take() function.\n",
    "# Store the result in task1_1_first_5_entries.\n",
    "\n",
    "task1_1_first_5_entries = ds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5131f59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30489067fffbb1a643e509dee200ad59",
     "grade": true,
     "grade_id": "cell-eef80ee1fe2f12b5",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Verified: each of the first 5 entries matches the saved output.\n"
     ]
    }
   ],
   "source": [
    "# read (NDJSON) and verify\n",
    "task1_1_path = os.path.expanduser(\"task1_1_expected_output.txt\")\n",
    "        \n",
    "with open(task1_1_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    expected_first_5_entries = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "assert len(task1_1_first_5_entries) == len(expected_first_5_entries) == 5, \"Expected 5 entries.\"\n",
    "\n",
    "for i, (got, exp) in enumerate(zip(task1_1_first_5_entries, expected_first_5_entries)):\n",
    "    assert got == exp, f\"Mismatch at index {i}: {got} != {exp}\"\n",
    "\n",
    "print(\"✅ Verified: each of the first 5 entries matches the saved output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a079b70",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Adding a column \n",
    "To add a column to a Ray Dataset, we use the ``Dataset.add_column()`` method, documentation for which can be found [here](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.add_column.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "011168ac",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54555b632fb99a74c44ebf54509d569b",
     "grade": false,
     "grade_id": "cell-7b14d96635bffe49",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:21,177\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_6_0\n",
      "2025-11-02 17:20:21,182\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_6_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:21,183\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_6_0: InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(add_ids)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aea3d8e18f740cc9d97770832ecdf92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af800ba6c55467384621804ba389269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(add_ids) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:21,452\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_6_0 execution finished in 0.27 seconds\n",
      "2025-11-02 17:20:21,474\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_8_0\n",
      "2025-11-02 17:20:21,476\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_8_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:21,476\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_8_0: InputDataBuffer[Input] -> LimitOperator[limit=5]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb10eec006284988826a2b03f6d5ffb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6e66c7cf6145dc959d520a7b535764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=5 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:21,500\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_8_0 execution finished in 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# add a column called id to your dataframe, where we number each of our entries\n",
    "# from 0 to ds.count()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def add_ids(batch):\n",
    "    batch[\"id\"] = np.arange(len(batch[\"unixReviewTime\"]))\n",
    "    return batch\n",
    "\n",
    "ds = ds.map_batches(add_ids, batch_size=100000)\n",
    "\n",
    "ds = ds.materialize() #why did we do this? Read the cell below.\n",
    "task_1_2_first_5_entries = ds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6436ded",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb88c55c1e8d3566ddec7788e5d54de5",
     "grade": true,
     "grade_id": "cell-a924c1d3cae400e6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Verified: each of the first 5 entries matches the saved output.\n"
     ]
    }
   ],
   "source": [
    "# read (NDJSON) and verify\n",
    "\n",
    "task1_2_path = os.path.expanduser(\"task1_2_expected_output.txt\")\n",
    "\n",
    "with open(task1_2_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    expected_first_5_entries = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "assert len(task_1_2_first_5_entries) == len(expected_first_5_entries) == 5, \"Expected 5 entries.\"\n",
    "\n",
    "for i, (got, exp) in enumerate(zip(task_1_2_first_5_entries, expected_first_5_entries)):\n",
    "    assert got == exp, f\"Mismatch at index {i}: {got} != {exp}\"\n",
    "\n",
    "print(\"✅ Verified: each of the first 5 entries matches the saved output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7804cc61",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lazy Execution in Ray \n",
    "\n",
    "As you may have noticed, we added a ``ds.materialize()`` command in the cell above. We do this because the default execution mode in Ray Data is Lazy and Streaming execution. You should read more about it [here](https://docs.ray.io/en/latest/data/data-internals.html#execution). We call ``materialize`` here to execute the ``add_column`` transformation on the entire dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec95c9e",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Compute Statistics\n",
    "Just like pandas, we can compute some statistics on our data using inbuilt functions like mean, min and max for columns in our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c647e4b",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78d2b227b7d8aec081e89c5b58008c2d",
     "grade": false,
     "grade_id": "cell-5e086083a4376fef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:21,524\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_10_0\n",
      "2025-11-02 17:20:21,525\tINFO hash_aggregate.py:180 -- Estimated memory requirement for aggregating aggregator (partitions=1, aggregators=1, dataset (estimate)=0.0GiB): shuffle=44.9MiB, output=44.9MiB, total=89.8MiB, \n",
      "2025-11-02 17:20:21,526\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_10_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:21,527\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_10_0: InputDataBuffer[Input] -> HashAggregateOperator[HashAggregate(key_columns=(), num_partitions=1)] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4480f0b0fb2e4664a6b156afc2060fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6667fcdaeb114dea92b50a4138c307d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- HashAggregate(key_columns=(), num_partitions=1) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd381cabcfc84ed7b1473ff16f7143d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle 2:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa60ecc49334d3a8968cb2438b2eb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregation 3:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8e9a01172d49738435d58a262a2062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=1 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:21,863\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_10_0 execution finished in 0.34 seconds\n",
      "2025-11-02 17:20:21,903\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_12_0\n",
      "2025-11-02 17:20:21,912\tINFO hash_aggregate.py:180 -- Estimated memory requirement for aggregating aggregator (partitions=1, aggregators=1, dataset (estimate)=0.0GiB): shuffle=44.9MiB, output=44.9MiB, total=89.8MiB, \n",
      "2025-11-02 17:20:21,913\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_12_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:21,914\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_12_0: InputDataBuffer[Input] -> HashAggregateOperator[HashAggregate(key_columns=(), num_partitions=1)] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1427670ea4e644ee8f8bf3fa7513ce40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4c5a5cf0624fc394f068928e023773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- HashAggregate(key_columns=(), num_partitions=1) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db209e60162742548b02234e411be677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle 2:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658b5bd31d404642bb70567f7578e28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregation 3:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150d2b07764f4ff9906c9819c3f3dad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=1 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:22,222\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_12_0 execution finished in 0.31 seconds\n",
      "2025-11-02 17:20:22,241\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_14_0\n",
      "2025-11-02 17:20:22,243\tINFO hash_aggregate.py:180 -- Estimated memory requirement for aggregating aggregator (partitions=1, aggregators=1, dataset (estimate)=0.0GiB): shuffle=44.9MiB, output=44.9MiB, total=89.8MiB, \n",
      "2025-11-02 17:20:22,244\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_14_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:22,244\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_14_0: InputDataBuffer[Input] -> HashAggregateOperator[HashAggregate(key_columns=(), num_partitions=1)] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299c1fe76a78439396b11e97d49939a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35793ae9fbf84d3aa1523878ea1f06ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- HashAggregate(key_columns=(), num_partitions=1) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed9e833d03a4178afbb45513ea1b073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle 2:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1985b2eb394b7c876fdfbfa23a3182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregation 3:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b22f0bfae14ebb80ddbe1c07bb4fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=1 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:22,543\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_14_0 execution finished in 0.30 seconds\n"
     ]
    }
   ],
   "source": [
    "# Calculate median of the overall rating, and mean of the vote count using\n",
    "# inbuilt Dataset methods. \n",
    "\n",
    "mean_overall = ds.mean(\"overall\")\n",
    "min_vote = ds.min(\"overall\")\n",
    "max_vote = ds.max(\"overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f6e95cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61d8c38139efa5016f20aa69ed404478",
     "grade": true,
     "grade_id": "cell-c3997dac13dd0c65",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Verified: mean_overall, min_vote, and max_vote match the saved output.\n"
     ]
    }
   ],
   "source": [
    "# --- read back and verify ---\n",
    "task1_3_path = os.path.expanduser(\"task1_3_expected_output.txt\")\n",
    "\n",
    "with open(task1_3_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    expected_metrics = [json.loads(line) for line in f if line.strip()][0]\n",
    "\n",
    "# recompute (or reuse the existing variables if you prefer)\n",
    "_mean_overall = mean_overall\n",
    "_min_vote = min_vote\n",
    "_max_vote = max_vote\n",
    "\n",
    "assert math.isclose(float(_mean_overall), float(expected_metrics[\"mean_overall\"]), rel_tol=1e-9, abs_tol=1e-12), \\\n",
    "    f\"mean_overall mismatch: {_mean_overall} != {expected_metrics['mean_overall']}\"\n",
    "assert math.isclose(float(_min_vote), float(expected_metrics[\"min_vote\"]), rel_tol=1e-9, abs_tol=1e-12), \\\n",
    "    f\"min_vote mismatch: {_min_vote} != {expected_metrics['min_vote']}\"\n",
    "assert math.isclose(float(_max_vote), float(expected_metrics[\"max_vote\"]), rel_tol=1e-9, abs_tol=1e-12), \\\n",
    "    f\"max_vote mismatch: {_max_vote} != {expected_metrics['max_vote']}\"\n",
    "\n",
    "print(\"✅ Verified: mean_overall, min_vote, and max_vote match the saved output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfc895",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Preprocessors in Ray Data\n",
    "\n",
    "Ray data is a part of the Ray AI Runtime system, and is built to be a scalable data processing library for ML applications. Hence, it has a rich library of various common preprocessors we require to use while serving ML models. [Here](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessor.Preprocessor.html#ray.data.preprocessor.Preprocessor) is how the inbuilt preprocessors work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b14a178",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8db8e151c4375ed8b87725320af02a8",
     "grade": false,
     "grade_id": "cell-79aac07c5a43da67",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:22,577\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_16_0\n",
      "2025-11-02 17:20:22,579\tINFO hash_aggregate.py:180 -- Estimated memory requirement for aggregating aggregator (partitions=1, aggregators=1, dataset (estimate)=0.0GiB): shuffle=44.9MiB, output=44.9MiB, total=89.8MiB, \n",
      "2025-11-02 17:20:22,581\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_16_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:22,581\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_16_0: InputDataBuffer[Input] -> HashAggregateOperator[HashAggregate(key_columns=(), num_partitions=1)] -> LimitOperator[limit=1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3662036a3f04e58bb53615bcf4164f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494768a31a2546dca29ec4ee568997ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- HashAggregate(key_columns=(), num_partitions=1) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bd62e161f141a2a1120897a35476a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle 2:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e881b12e0a244f258e2cfd6fe8b160a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregation 3:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e142f932bf43a0832d396d5a19c172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=1 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:22,977\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_16_0 execution finished in 0.40 seconds\n",
      "2025-11-02 17:20:23,021\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_18_0\n",
      "2025-11-02 17:20:23,024\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_18_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:23,030\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_18_0: InputDataBuffer[Input] -> TaskPoolMapOperator[MaxAbsScaler]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f450fc8a09374846a79c68f0bf59303b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7f2713604b4833bc8ac10372b039e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MaxAbsScaler 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:23,158\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_18_0 execution finished in 0.13 seconds\n",
      "2025-11-02 17:20:23,174\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_20_0\n",
      "2025-11-02 17:20:23,175\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_20_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:23,175\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_20_0: InputDataBuffer[Input] -> LimitOperator[limit=5]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5c4c918af848a1afb609a3f8d28984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ebba9b16144fec9bb7cfa2420c82aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=5 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:23,248\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_20_0 execution finished in 0.07 seconds\n"
     ]
    }
   ],
   "source": [
    "# Scale each 'overall' using it's maximum absolute value using the MaxAbsScaler\n",
    "\n",
    "from ray.data.preprocessors import MaxAbsScaler\n",
    "\n",
    "scaler = MaxAbsScaler(columns=[\"overall\"])\n",
    "scaler = scaler.fit(ds)\n",
    "ds = scaler.transform(ds)\n",
    "ds = ds.materialize()\n",
    "\n",
    "task1_4_first_5_entries = ds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d401136a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51da6edad7a79dc31e49f62be1dda39a",
     "grade": true,
     "grade_id": "cell-0398c50aefd12d32",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Verified: each of the first 5 entries matches the saved output.\n"
     ]
    }
   ],
   "source": [
    "# read (NDJSON) and verify\n",
    "task1_4_path = os.path.expanduser(\"task1_4_expected_output.txt\")\n",
    "\n",
    "with open(task1_4_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    expected_first_5_entries = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "assert len(task1_4_first_5_entries) == len(expected_first_5_entries) == 5, \"Expected 5 entries.\"\n",
    "\n",
    "for i, (got, exp) in enumerate(zip(task1_4_first_5_entries, expected_first_5_entries)):\n",
    "    assert got == exp, f\"Mismatch at index {i}: {got} != {exp}\"\n",
    "\n",
    "print(\"✅ Verified: each of the first 5 entries matches the saved output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0e5de",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Applying a transform over the entire dataset\n",
    "\n",
    "To apply a function to the entire dataset, we use the ``Dataset.map()`` method. It transforms the dataset row-wise in accordance to the function you pass into it. Dataset.map uses ray tasks to transform the blocks of the dataset. [Here](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html#ray.data.Dataset.map) is an example of how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c21d8756",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1279ca4719782aa8e0964f4083c6f1cd",
     "grade": false,
     "grade_id": "cell-ec601c4069de103a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:23,291\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_22_0\n",
      "2025-11-02 17:20:23,293\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_22_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:23,293\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_22_0: InputDataBuffer[Input] -> LimitOperator[limit=5] -> TaskPoolMapOperator[Map(lowercase)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de610e04bb1e4dada63fa88ce7d2309b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1f9c83bab74ed28b72b521d1b06334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=5 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495cb2424e96427c9fe0023ec05f19b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(lowercase) 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:23,386\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_22_0 execution finished in 0.09 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create a function named lowercase() that accepts a single row of data as\n",
    "# input and converts the text in the 'summary' column of each row to lowercase\n",
    "# letters Use Dataset.map() to apply this function over the entire dataset.\n",
    "\n",
    "# Define a row-wise lowercase() transform and apply with Dataset.map()\n",
    "def lowercase(row):\n",
    "    s = row.get(\"summary\")\n",
    "    if isinstance(s, str):\n",
    "        row[\"summary\"] = s.lower()\n",
    "    return row\n",
    "\n",
    "ds = ds.map(lowercase)\n",
    "\n",
    "task1_5_first_5_entries = ds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2b79378",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d581b4fdbefb40be67aa3cc6133fd220",
     "grade": true,
     "grade_id": "cell-cbcf27775b574a78",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Verified: each of the first 5 entries matches the saved output.\n"
     ]
    }
   ],
   "source": [
    "# read (NDJSON) and verify\n",
    "task1_5_path = os.path.expanduser(\"task1_5_expected_output.txt\")\n",
    "\n",
    "with open(task1_5_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    expected_first_5_entries = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "assert len(task1_5_first_5_entries) == len(expected_first_5_entries) == 5, \"Expected 5 entries.\"\n",
    "\n",
    "for i, (got, exp) in enumerate(zip(task1_5_first_5_entries, expected_first_5_entries)):\n",
    "    assert got == exp, f\"Mismatch at index {i}: {got} != {exp}\"\n",
    "\n",
    "print(\"✅ Verified: each of the first 5 entries matches the saved output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194d7f2",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Applying a vectorized transformation over the entire dataset\n",
    "\n",
    "If your transformation can be vectorized, i.e applied to multiple rows at ones, you can apply that transform over batches. You do so using the ``Dataset.map_batches()`` method. [Here](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) is an example how to use it. \n",
    "\n",
    "Hint: set the parameter ``batch_format=\"pandas\"`` for this dataset while using``map_batches()`` to avoid Ray-data specific issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fba3544",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37544b3c8ff49fdb33621085ddc816f4",
     "grade": false,
     "grade_id": "cell-215aac826778b190",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:20:23,413\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_24_0\n",
      "2025-11-02 17:20:23,415\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_24_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:20:23,415\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_24_0: InputDataBuffer[Input] -> TaskPoolMapOperator[Map(lowercase)->MapBatches(scale)] -> LimitOperator[limit=5]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a09d28a2dd648199f2786769452a4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224b3d242d2643a8879d0f89227c6252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(lowercase)->MapBatches(scale) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fe5141056846d1b86a4f8e69a1876f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=5 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:21:52,456\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_24_0 execution finished in 89.04 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def scale(batch: pd.DataFrame):\n",
    "    # Scale from [0, 1] to [-1, 1]\n",
    "    # Formula: new_value = 2 * old_value - 1\n",
    "    batch[\"overall\"] = 2 * batch[\"overall\"] - 1\n",
    "    return batch\n",
    "\n",
    "ds = ds.map_batches(scale, batch_size=128, batch_format=\"pandas\")\n",
    "\n",
    "task1_6_first_5_entries = ds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0884196",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5079d72ca2a051ada251f1229f9b13a6",
     "grade": true,
     "grade_id": "cell-36ebcc4f1a2babe1",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Verified: each of the first 5 entries matches the saved output.\n"
     ]
    }
   ],
   "source": [
    "# read (NDJSON) and verify\n",
    "task1_6_path = os.path.expanduser(\"task1_6_expected_output.txt\")\n",
    "\n",
    "with open(task1_6_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    expected_first_5_entries = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "assert len(task1_6_first_5_entries) == len(expected_first_5_entries) == 5, \"Expected 5 entries.\"\n",
    "\n",
    "for i, (got, exp) in enumerate(zip(task1_6_first_5_entries, expected_first_5_entries)):\n",
    "    assert got == exp, f\"Mismatch at index {i}: {got} != {exp}\"\n",
    "\n",
    "print(\"✅ Verified: each of the first 5 entries matches the saved output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff10a8aa",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Cleaning up reviewText\n",
    "\n",
    "Write a function called ``preprocessor()`` which takes in a batch of size 128. You should convert the ``reviewText``  in each row into lowercase letters, remove all punctuation (we suggest using regex), and tokenize the sentence. A GPT-2 tokenizer has been instantiated for you, and you should use the tokenizer.encode() method to tokenize each `reviewText`. Add these tokenized representations to your dataset under the column ``tokenizedText``. \n",
    "(You might have to add the new column beforehand)\n",
    "\n",
    "Apply this preprocessor transform using map_batches.\n",
    "Print the first 5 entries of this transformed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f485f432",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9531b32",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43c36295db6eeb187ef2a24ee5e4992c",
     "grade": false,
     "grade_id": "cell-24c00c24eee1435d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:21:55,058\tINFO logging.py:293 -- Registered dataset logger for dataset dataset_27_0\n",
      "2025-11-02 17:21:55,060\tINFO streaming_executor.py:159 -- Starting execution of Dataset dataset_27_0. Full logs are in /tmp/ray/session_2025-11-02_17-20-16_753217_4187/logs/ray-data\n",
      "2025-11-02 17:21:55,060\tINFO streaming_executor.py:160 -- Execution plan of Dataset dataset_27_0: InputDataBuffer[Input] -> TaskPoolMapOperator[Map(lowercase)->MapBatches(scale)->MapBatches(add_column)->MapBatches(preprocessor)] -> LimitOperator[limit=5]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a4452b7c2f4504a2409d5640a0b396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e1f85cdde74e2eadd7b49571e42357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(lowercase)->MapBatches(scale)->MapBatches(add_column)->MapBatches(preprocessor) 1: 0.00 row [00:00, ? ro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3fe51f3a2149c9ac9b33809de30a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=5 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 17:21:55,110\tWARNING progress_bar.py:120 -- Truncating long operator name to 100 characters. To disable this behavior, set `ray.data.DataContext.get_current().DEFAULT_ENABLE_PROGRESS_BAR_NAME_TRUNCATION = False`.\n",
      "2025-11-02 17:23:29,696\tINFO streaming_executor.py:279 -- ✔️  Dataset dataset_27_0 execution finished in 94.63 seconds\n"
     ]
    }
   ],
   "source": [
    "# Write a `preprocessor` function that can tokenize text for a batch of data. \n",
    "# Store the result of the map_batches in `transformed`. Use map_batches again.\n",
    "\n",
    "ds = ds.add_column(\"tokenizedText\", lambda df: None)\n",
    "\n",
    "def preprocessor(batch: pd.DataFrame):\n",
    "    import re\n",
    "    \n",
    "    # Process each review text\n",
    "    processed_texts = []\n",
    "    for text in batch[\"reviewText\"]:\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove punctuation using regex\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        processed_texts.append(text)\n",
    "    \n",
    "    # Update the reviewText column\n",
    "    batch[\"reviewText\"] = processed_texts\n",
    "    \n",
    "    # Tokenize each text\n",
    "    tokenized = []\n",
    "    for text in processed_texts:\n",
    "        tokens = tokenizer.encode(text)\n",
    "        tokenized.append(tokens)\n",
    "    \n",
    "    # Update the tokenizedText column\n",
    "    batch[\"tokenizedText\"] = tokenized\n",
    "    \n",
    "    return batch\n",
    "\n",
    "transformed = ds.map_batches(preprocessor, batch_size=128, batch_format=\"pandas\")\n",
    "\n",
    "transformed_results = transformed.take(5)\n",
    "decode_txt = tokenizer.decode(transformed_results[0][\"tokenizedText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77bf3a5b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cfd557d53808981ba2c6c23ea9a00b1",
     "grade": true,
     "grade_id": "cell-0516fe45e4c430fb",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ decode_txt round-trip matches expected output\n"
     ]
    }
   ],
   "source": [
    "# Check if you have tokenized your text correctly \n",
    "\n",
    "assert decode_txt == transformed_results[0]['reviewText'] \n",
    "\n",
    "task1_7_path = os.path.expanduser(\"task1_7_expected_output.txt\")\n",
    "\n",
    "with open(task1_7_path, \"r\", encoding=\"utf-8\") as f: \n",
    "    content = f.read()\n",
    "assert content == decode_txt\n",
    "print(\"✅ decode_txt round-trip matches expected output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f295286a-22db-4b53-b3a5-d2062f0797a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339c28e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Task 3: Mixture of Experts with Ray\n",
    "\n",
    "This assignment focuses on implementing a distributed Mixture of Experts (MoE) model using Ray. To help you understand the workflow, we provide a reference implementation called `SimpleMoE`. You will then complete the skeleton for the `MoE_TP` class, which incorporates tensor parallelism (TP) into MoE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de116647",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f3567b714cd5e141032dea09e801592",
     "grade": false,
     "grade_id": "cell-401d417baa84d487",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ray\n",
    "from rng import get_rng, rng_context, reset_rngs\n",
    "import time\n",
    "ray.init(ignore_reinit_error=True, num_cpus=8)\n",
    "\n",
    "# For simplicity, we assume that `hidden_dim` and `output_dim`\n",
    "# are evenly divisible by `num_workers`.\n",
    "params={\n",
    "    \"batch_size\": 1000,\n",
    "    \"feature_dim\": 1000,\n",
    "    \"hidden_dim\": 1000,\n",
    "    \"output_dim\": 1000,\n",
    "    \"num_experts\": 10,\n",
    "    \"topk\": 2,\n",
    "}\n",
    "num_workers = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed21c5-d7de-4943-8140-711152205d5c",
   "metadata": {},
   "source": [
    "## Simple_MoE\n",
    "\n",
    "Mixture of Experts (MoE) is a neural network architecture where multiple “experts” (sub-networks) exist in parallel. A gating network selects which experts should process each input, and `top_k` specifies how many experts are chosen. This approach can greatly increase model capacity without a proportional increase in computation per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c28d2d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e450f16ee8cebf00d931cd452867f278",
     "grade": false,
     "grade_id": "cell-39275dfc1a52fb1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Simple linear layer y = xW + b\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.weight = get_rng().randn(in_features, out_features) * 0.01\n",
    "        self.bias = get_rng().randn(out_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "\n",
    "\n",
    "class Expert:\n",
    "    \"\"\"Expert network with one hidden layer and ReLU activation\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.fc1 = Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        hidden = np.maximum(0, hidden)  # ReLU\n",
    "        return self.fc2(hidden)\n",
    "\n",
    "\n",
    "class Router:\n",
    "    \"\"\"Routes inputs to experts using softmax-based gating\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        self.linear = Linear(input_dim, num_experts)\n",
    "\n",
    "    def __call__(self, x, topk=1):\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        # Softmax for routing probabilities\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "        # Select top-k experts\n",
    "        indices = np.argsort(-probs, axis=1)[:, :topk]\n",
    "        gates = np.take_along_axis(probs, indices, axis=1)\n",
    "\n",
    "        # Normalize gates to sum to 1\n",
    "        gates = gates / np.sum(gates, axis=1, keepdims=True)\n",
    "\n",
    "        return indices, gates\n",
    "\n",
    "\n",
    "class SimpleMoE:\n",
    "    \"\"\"\n",
    "    Simple reference implementation of Mixture of Experts.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Input feature dimension\n",
    "        hidden_dim (int): Hidden dimension for each expert\n",
    "        output_dim (int): Output dimension\n",
    "        num_experts (int): Number of expert networks\n",
    "        topk (int): Number of experts to route each input to\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts, topk=1):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.topk = min(topk, num_experts)\n",
    "\n",
    "        with rng_context('router'):\n",
    "            self.router = Router(input_dim, num_experts)\n",
    "\n",
    "        with rng_context('expert'):\n",
    "            self.experts = [Expert(input_dim, hidden_dim, output_dim)\n",
    "                            for _ in range(num_experts)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        indices, gates = self.router(x, self.topk)\n",
    "        outputs = np.zeros((batch_size, self.output_dim))\n",
    "\n",
    "        for k in range(self.topk):\n",
    "            for i in range(batch_size):\n",
    "                expert_idx = indices[i, k]\n",
    "                gate = gates[i, k]\n",
    "                item = x[i:i + 1]\n",
    "                expert_output = self.experts[expert_idx](item)\n",
    "                outputs[i] += gate * expert_output[0]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c5d64",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88a64037fc742d6c041e5adade37d2be",
     "grade": false,
     "grade_id": "cell-329710a0038adb20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_simple_moe(batch_size=10, feature_dim=10, hidden_dim=10, output_dim=10, num_experts=1, topk=1):\n",
    "    \"\"\"Test SimpleMoE for correctness and performance\"\"\"\n",
    "    reset_rngs()\n",
    "    \n",
    "    # Generate input data\n",
    "    with rng_context(\"testing\"):\n",
    "        X = get_rng().randn(batch_size, feature_dim)\n",
    "    \n",
    "    moe = SimpleMoE(\n",
    "        input_dim=feature_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_experts=num_experts,\n",
    "        topk=topk\n",
    "    )\n",
    "    \n",
    "    # Warm up to better measure efficiency\n",
    "    _ = moe(X)\n",
    "    \n",
    "    # Measure time\n",
    "    start_time = time.time()\n",
    "    output = moe(X)\n",
    "    end_time = time.time()\n",
    "    avg_duration_ms = end_time - start_time\n",
    "    \n",
    "    return output, avg_duration_ms\n",
    "\n",
    "result_simple, time_simple = test_simple_moe(**params)\n",
    "print(f\"Simple MoE:\")\n",
    "print(f\"  Avg time: {time_simple:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa34ac5",
   "metadata": {},
   "source": [
    "## Task 3.1: MoE with Tensor Parallel\n",
    "\n",
    "**Tensor Parallelism (TP)** is a distributed training technique where a single layer (e.g., a large matrix multiplication) is split across multiple devices. Each device computes a partial result, and the partial results are combined to produce the full output.\n",
    "\n",
    "In **MoE with Tensor Parallelism (MoE-TP)**, these ideas are combined:\n",
    "\n",
    "- Each expert consists of multiple layers, and the parameters of every layer are sharded across all workers (i.e., each worker stores only a slice of the layer’s weights).  \n",
    "  In this assignment, we assume there are 10 experts, each containing 2 fully connected layers, for a total of 20 layers. Under tensor parallelism, each worker maintains a shard from every one of these 20 layers.\n",
    "\n",
    "- Every worker computes its partial output for each layer (based on the parameters it stores).\n",
    "\n",
    "- The partial outputs are then combined by concatenation to obtain the full expert outputs. See `ShardedLinear` class for how a sharded linear layer works.\n",
    "\n",
    "- This design is well-suited for balanced workloads where all experts need to be computed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f829cd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c613017518da8a465a33aa69586294b",
     "grade": false,
     "grade_id": "cell-00899cfb78084bbe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class LinearShardWorker:\n",
    "    \"\"\"\n",
    "    Generic Ray worker that holds partial weight shards for all experts \n",
    "    Each fc layer from each expert is represented by a unique layer_id.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rank, world_size):\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.layers = {}  # Store weights for multiple layers by layer_id\n",
    "    \n",
    "    def initialize_layer(self, layer_id, weight_shard, bias_shard):\n",
    "        \"\"\"\n",
    "        Store weight shards for a specific layer.\n",
    "        \n",
    "        Args:\n",
    "            layer_id: Unique identifier for this layer\n",
    "            weight_shard: Pre-computed weight shard for this worker\n",
    "            bias_shard: Pre-computed bias shard for this worker\n",
    "        \"\"\"\n",
    "        local_out_features = weight_shard.shape[1]\n",
    "        self.layers[layer_id] = {'weight': weight_shard, 'bias': bias_shard, 'local_out_features': local_out_features}\n",
    "        return local_out_features\n",
    "    \n",
    "    def forward_layer(self, layer_id, x):\n",
    "        \"\"\"\n",
    "        Compute local shard of a layer's output.\n",
    "        \n",
    "        Args:\n",
    "            layer_id: Identifier for which layer to use\n",
    "            x: Input of shape (batch_size, in_features)\n",
    "            \n",
    "        Returns:\n",
    "            Local output shard of shape (batch_size, out_features // world_size)\n",
    "        \"\"\"\n",
    "        layer = self.layers[layer_id]\n",
    "        if x.shape[0] == 0:\n",
    "            return np.zeros((0, layer['local_out_features']))\n",
    "        \n",
    "        # Perform local computation\n",
    "        local_output = np.dot(x, layer['weight']) + layer['bias']\n",
    "        return local_output\n",
    "\n",
    "\n",
    "class ShardedLinear:\n",
    "    \"\"\"\n",
    "    Linear layer that is sharded across Ray workers.\n",
    "    \n",
    "    Each worker holds a shard, __call__ dispatches + concatenates\n",
    "    \"\"\"\n",
    "\n",
    "    # _layer_counter: Class-level counter for unique ShardedLinear IDs\n",
    "    # (In MoE_TP, you have multiple experts (e.g., 10 experts), each expert has 2 layers (fc1 and fc2),\n",
    "    # and all layers share the same workers. So the same Ray worker holds shards for:\n",
    "    # - Expert 0's fc1\n",
    "    # - Expert 0's fc2\n",
    "    # - Expert 1's fc1\n",
    "    # - Expert 1's fc2\n",
    "    # - ... 20 layers total for 10 experts!)\n",
    "    _layer_counter = 0\n",
    "    \n",
    "    def __init__(self, in_features, out_features, workers):\n",
    "        self.workers = workers\n",
    "        self.world_size = len(workers)\n",
    "        \n",
    "        # Assert that out_features is evenly divisible by world_size\n",
    "        assert out_features % self.world_size == 0, \\\n",
    "            f\"Output features ({out_features}) must be evenly divisible by world size ({self.world_size})\"\n",
    "        \n",
    "        # Calculate the local output dimension\n",
    "        self.out_features_global = out_features\n",
    "        self.local_out_features = out_features // self.world_size\n",
    "        \n",
    "        # Assign unique layer ID\n",
    "        self.layer_id = f\"layer_{ShardedLinear._layer_counter}\"\n",
    "        ShardedLinear._layer_counter += 1\n",
    "        \n",
    "        # Generate full weights, then shard them\n",
    "        # This ensures each shard corresponds to the same weights as in SimpleMoE \n",
    "        # generated by random number generator (rng). \n",
    "        full_weight = get_rng().randn(in_features, out_features) * 0.01\n",
    "        full_bias = get_rng().randn(out_features)\n",
    "        \n",
    "        # Distribute shards to workers\n",
    "        futures = []\n",
    "        for rank, worker in enumerate(self.workers):\n",
    "            offset = rank * self.local_out_features\n",
    "\n",
    "            ### Compute weight_shard and bias_shard, then call worker.initialize_layer.remote\n",
    "            weight_shard = full_weight[:, offset:offset + self.local_out_features]\n",
    "            bias_shard = full_bias[offset:offset + self.local_out_features]\n",
    "            futures.append(\n",
    "                worker.initialize_layer.remote(self.layer_id, weight_shard, bias_shard)\n",
    "            )\n",
    "        ray.get(futures)  # Wait for initialization\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through sharded linear layer.\n",
    "        \"\"\"\n",
    "        # Handle empty batch case\n",
    "        if x.shape[0] == 0:\n",
    "            return np.zeros((0, self.out_features_global))\n",
    "        \n",
    "        # Compute partial output from each worker\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        shards = ray.get(futures)\n",
    "        \n",
    "        # Concatenate shards\n",
    "        result = np.concatenate(shards, axis=1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class ShardedExpert:\n",
    "    \"\"\"\n",
    "    Expert network with one hidden layer and ReLU activation, sharded across workers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, workers):\n",
    "        # Initialize two ShardedLinear layers\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        hidden = np.maximum(0, hidden)  # ReLU\n",
    "        return self.fc2(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24484d57",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9067f25298a2fe4aa65e1c4b16b6741b",
     "grade": false,
     "grade_id": "cell-9b19daf5ffb89676",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MoE_TP:\n",
    "    \"\"\"\n",
    "    Distributed Mixture of Experts using Ray for tensor parallelism.\n",
    "    \n",
    "    TP-style MoE:\n",
    "    - Each worker holds a portion of every expert (sharded experts)\n",
    "    - Router is replicated\n",
    "    - ShardedExpert classes coordinate worker communication\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Input feature dimension\n",
    "        hidden_dim (int): Hidden dimension for each expert\n",
    "        output_dim (int): Output dimension\n",
    "        num_experts (int): Total number of experts in the model\n",
    "        num_workers (int): Number of parallel workers\n",
    "        topk (int): Number of experts to route each input to\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts, num_workers=4, topk=1):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.topk = min(topk, num_experts)\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Validate dimensions\n",
    "        assert hidden_dim % num_workers == 0, \\\n",
    "            f\"hidden_dim ({hidden_dim}) must be divisible by num_workers ({num_workers})\"\n",
    "        assert output_dim % num_workers == 0, \\\n",
    "            f\"output_dim ({output_dim}) must be divisible by num_workers ({num_workers})\"\n",
    "        \n",
    "        # Initialize Ray if not already initialized\n",
    "        if not ray.is_initialized():\n",
    "            ray.init(ignore_reinit_error=True)\n",
    "        \n",
    "        # Create router\n",
    "        with rng_context('router'):\n",
    "            self.router = Router(input_dim, num_experts)\n",
    "        \n",
    "        # Each worker will hold shards of all experts\n",
    "        self.workers = [\n",
    "            LinearShardWorker.remote(rank, num_workers)\n",
    "            for rank in range(num_workers)\n",
    "        ]\n",
    "\n",
    "        # Create sharded experts - each expert is sharded across all workers\n",
    "        with rng_context('expert'):\n",
    "            self.experts = [\n",
    "                ShardedExpert(input_dim, hidden_dim, output_dim, self.workers)\n",
    "                for _ in range(num_experts)\n",
    "            ]\n",
    "        \n",
    "        print(f\"Initialized MoE_TP with {num_experts} experts, each sharded across {num_workers} workers\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Distributed forward pass through the MoE model using tensor parallelism.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # All processes compute routing (router is replicated)\n",
    "        indices, gates = self.router(x, self.topk)\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        outputs = np.zeros((batch_size, self.output_dim))\n",
    "\n",
    "        # Process one expert at a time\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            # Find all batch_idx combinations that route to this expert, and their corresponding gates\n",
    "            batch_indices = []\n",
    "            expert_gates = []\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            # If no samples are routed to this expert, skip it\n",
    "            if not batch_indices:\n",
    "                continue\n",
    "                \n",
    "            # Create a batch of inputs for this expert\n",
    "            expert_inputs = x[batch_indices]\n",
    "            expert_gates = np.array(expert_gates)[:, np.newaxis]  # Shape: (num_samples, 1)\n",
    "            \n",
    "            # Process expert_inputs through this expert\n",
    "            # Then scale outputs by their gates.\n",
    "            # Add the gated outputs to the result tensor (outputs) based on batch_indices\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def shutdown(self):\n",
    "        \"\"\"Cleanup Ray actors\"\"\"\n",
    "        for worker in self.workers:\n",
    "            ray.kill(worker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7da1d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b39d6e3c0fb52481907d5211cb93be6a",
     "grade": true,
     "grade_id": "cell-c7de210645c6337a",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_tp_moe(batch_size=10, feature_dim=10, hidden_dim=10, output_dim=10, num_experts=1, num_workers=1, topk=1):\n",
    "    \"\"\"Test MoE_TP for correctness and performance\"\"\"\n",
    "    reset_rngs()\n",
    "    \n",
    "    # Generate input data\n",
    "    with rng_context(\"testing\"):\n",
    "        X = get_rng().randn(batch_size, feature_dim)\n",
    "    \n",
    "    moe = MoE_TP(\n",
    "        input_dim=feature_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_experts=num_experts,\n",
    "        num_workers=num_workers,\n",
    "        topk=topk\n",
    "    )\n",
    "    \n",
    "    # Warm up to better measure efficiency\n",
    "    _ = moe(X)\n",
    "    \n",
    "    # Measure time\n",
    "    start_time = time.time()\n",
    "    output = moe(X)\n",
    "    end_time = time.time()\n",
    "    avg_duration_ms = end_time - start_time\n",
    "    \n",
    "    # Cleanup\n",
    "    if hasattr(moe, 'shutdown'):\n",
    "        moe.shutdown()\n",
    "    \n",
    "    return output, avg_duration_ms\n",
    "\n",
    "\n",
    "result_tp, time_tp = test_tp_moe(**params, num_workers=num_workers)\n",
    "print(f\"\\nTP MoE:\")\n",
    "print(f\"  Avg time: {time_tp:.2f} ms\")\n",
    "assert time_simple / time_tp > 2, f\"Tensor Parallel MoE time not efficient compared with Simple MoE.\"\n",
    "print(f\"  ✅ Efficiency test passed, Speed up > 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb6c756",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c1c4aafc883d9fdc3b334ec41893d60",
     "grade": true,
     "grade_id": "cell-b79dfa9c288e5834",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(result_tp.sum() - result_simple.sum()) <= 0.1, f\"Tensor Parallel MoE test failed: {abs(result_tp.sum() - result_simple.sum())}\"\n",
    "print(f\"  ✅ Correctness test passed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

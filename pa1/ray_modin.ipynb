{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a2b685-7b9a-4685-a628-96c0b7ae2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Set up Jupyter Kernel, select Python 3 (ipykernel).\n",
    "# ## Only need to run once.\n",
    "# ## restart the kernel after finishing installation\n",
    "\n",
    "# !pip install \"ray==2.10.0\"\n",
    "# !pip install \"pyarrow==18.1.0\"\n",
    "# !pip install \"modin==0.37.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8247d15f-013e-433d-85fb-98e90d76d9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 2.10.0\n",
      "modin: 0.37.0\n"
     ]
    }
   ],
   "source": [
    "## Verify installations:\n",
    "!python -c \"import ray; print('ray:', ray.__version__)\"\n",
    "!python -c \"import modin; print('modin:', modin.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c18a87-8d9d-4fe3-82f2-228a2393fce5",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "#### Task 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba4f614-1092-4582-ba3e-af33a3be0696",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c95b503016819a678c6c103ab27bd61a",
     "grade": false,
     "grade_id": "cell-14904bc93b5034ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 20:37:19,212\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# Note: We include only 10M data in the csv file to fit the memory of Datahub.\n",
    "\n",
    "import ray, json, os\n",
    "import time\n",
    "import numpy as np\n",
    "import modin.pandas as pd\n",
    "from modin.utils import reload_modin\n",
    "ray.shutdown()\n",
    "reload_modin()\n",
    "ray.init(num_cpus=4)\n",
    "\n",
    "def run_task2(path):\n",
    "    ## DO NOT MODIFY: START \n",
    "    start_time = time.perf_counter()\n",
    "    raw_df = pd.read_csv(path)\n",
    "    ## DO NOT MODIFY: End \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Compact vote parsing: normalize missing, convert comma-decimals, remove thousands commas,\n",
    "    # coerce to numeric, fill missing with 0, and clip extreme outliers at the 99th percentile.\n",
    "    v = raw_df['vote'].replace('-', pd.NA).astype(str).str.strip()\n",
    "    v = v.str.replace(r',(?=\\d{1,2}$)', '.', regex=True).str.replace(',', '', regex=False)\n",
    "    v = v.replace({'nan': None, '': None})\n",
    "    # Convert to numeric and keep NaN for missing values so mean() ignores them\n",
    "    raw_df['vote'] = pd.to_numeric(v, errors='coerce')\n",
    "    raw_df['rating_bucket'] = raw_df['overall'].round().astype('Int64')\n",
    "    # Convert unixReviewTime to year\n",
    "    raw_df['reviewYear'] = pd.to_datetime(raw_df['unixReviewTime'], unit='s').dt.year\n",
    "\n",
    "    # Treat missing votes as 0 for the avg_helpful_votes_per_review calculation\n",
    "    raw_df['vote_filled'] = raw_df['vote'].fillna(0.0)\n",
    "    output = raw_df.groupby('rating_bucket').agg(\n",
    "    \tn_reviews=('reviewerID', 'size'),\n",
    "    \tn_unique_reviewers=('reviewerID', 'nunique'),\n",
    "    \tsum_helpful_votes=('vote_filled', 'sum'),\n",
    "    \tearliest_review_year=('reviewYear', 'min')\n",
    "    ).reset_index()\n",
    "    # compute average helpful votes per review using sum / n_reviews (includes missing as zeros)\n",
    "    output['avg_helpful_votes_per_review'] = output['sum_helpful_votes'] / output['n_reviews']\n",
    "    output = output.drop(columns=['sum_helpful_votes'])\n",
    "    \n",
    "    ## DO NOT MODIFY: START \n",
    "    submit = output.describe().round(2)\n",
    "    duration_s = time.perf_counter() - start_time\n",
    "    print(f\"Processing time (excluding file write): {duration_s:.3f} s\")\n",
    "    current_data = json.loads(submit._to_pandas().to_json())\n",
    "    return current_data\n",
    "    ## DO NOT MODIFY: END "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165336e7-f656-465e-850d-60cdf0bc2241",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69eb7e84ca9c7da65a00c5380d7de24a",
     "grade": false,
     "grade_id": "cell-2608234f116dad98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compare_json_files(current_data):\n",
    "        # Load both JSON files\n",
    "    path = os.path.expanduser(\"~/public/origin_results_PA1_task2_1.json\")\n",
    "    with open(path, 'r') as f:\n",
    "        origin_data = json.load(f)\n",
    "        \n",
    "    # Track if any error > 1% exists\n",
    "    has_error_over_1_percent = False\n",
    "    \n",
    "    # Compare each metric\n",
    "    for metric in origin_data.keys():\n",
    "        print(f\"\\n=== Comparing {metric} ===\")\n",
    "        \n",
    "        for stat in origin_data[metric].keys():\n",
    "            origin_val = origin_data[metric][stat]\n",
    "            current_val = current_data[metric][stat]\n",
    "            \n",
    "            # Calculate error percentage\n",
    "            if origin_val != 0:\n",
    "                error_percent = abs(current_val - origin_val) / abs(origin_val) * 100\n",
    "            else:\n",
    "                error_percent = 0 if current_val == 0 else float('inf')\n",
    "            \n",
    "            # Only print if error > 1%\n",
    "            if error_percent > 1.0:\n",
    "                print(f\"  {stat}: {current_val} vs {origin_val} (error: {error_percent:.2f}%)\")\n",
    "                has_error_over_1_percent = True\n",
    "    \n",
    "    # Assert if there exists an error > 1%\n",
    "    assert not has_error_over_1_percent, \"Found errors greater than 1% between the JSON files\"\n",
    "    \n",
    "    if not has_error_over_1_percent:\n",
    "        print(\"\\n✓ All comparisons within 1% tolerance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "046f6370-0043-499a-bbe7-47ff7a4a9170",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26e68e4a863549cdd1e80952b44e6011",
     "grade": true,
     "grade_id": "cell-58b91a1d107d3ac7",
     "locked": true,
     "points": 40,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time (excluding file write): 25.945 s\n",
      "\n",
      "=== Comparing rating_bucket ===\n",
      "\n",
      "=== Comparing n_reviews ===\n",
      "\n",
      "=== Comparing n_unique_reviewers ===\n",
      "\n",
      "=== Comparing avg_helpful_votes_per_review ===\n",
      "\n",
      "=== Comparing earliest_review_year ===\n",
      "\n",
      "✓ All comparisons within 1% tolerance\n"
     ]
    }
   ],
   "source": [
    "raw_dataset_path = \"~/public/modin_dev_dataset_10M_rows.csv\"\n",
    "current_data = run_task2(raw_dataset_path)\n",
    "compare_json_files(current_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501757f-ae1b-4c4c-b6c8-478296d13200",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "caab3e41f1728bc8069ade75b165b69d",
     "grade": false,
     "grade_id": "cell-5e7f49b02bcf9957",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 2.2\n",
    "Change the number of CPUs used by Modin or the Ray backend ([documentation here](https://modin.readthedocs.io/en/stable/getting_started/using_modin/using_modin_locally.html#advanced-configuring-the-resources-modin-uses)) on your instance and run your data manipulation code. Document the execution times you see with 1, 2, 3, and 4 CPUs. Is it a linear speedup? If not, why?\n",
    "\n",
    "Note:\n",
    "* We won't check the processing time for Task2. So in your submission, it doesn't matter how many CPUs you set in ray.init(num_cpus=X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59514e0-08cb-4556-a5a2-0cc26eb8e88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 20:37:50,699\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time (excluding file write): 71.143 s\n",
      "CPUs=1: 71.15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 20:39:07,385\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time (excluding file write): 39.509 s\n",
      "CPUs=2: 39.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 20:39:52,348\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time (excluding file write): 29.599 s\n",
      "CPUs=3: 29.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 20:40:27,446\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time (excluding file write): 25.067 s\n",
      "CPUs=4: 25.07s\n",
      "\n",
      "Timings: {1: 71.15424335561693, 2: 39.51682998565957, 3: 29.607038429006934, 4: 25.074957669246942}\n",
      "CPUs=1: speedup=1.00 (ideal=1, efficiency=100.0%)\n",
      "CPUs=2: speedup=1.80 (ideal=2, efficiency=90.0%)\n",
      "CPUs=3: speedup=2.40 (ideal=3, efficiency=80.1%)\n",
      "CPUs=4: speedup=2.84 (ideal=4, efficiency=70.9%)\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "from modin.utils import reload_modin\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _reconfigure_ray(num_cpus: int):\n",
    "    \"\"\"Rebind Modin to a fresh Ray cluster with the requested CPU count.\"\"\"\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "    reload_modin()\n",
    "    ray.init(num_cpus=num_cpus)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        # keep cluster alive for interactive use\n",
    "        pass\n",
    "\n",
    "\n",
    "def measure_task2_runtimes(cpus=(1, 2, 3, 4)):\n",
    "    timings = {}\n",
    "    for c in cpus:\n",
    "        with _reconfigure_ray(c):\n",
    "            t0 = time.perf_counter()\n",
    "            _ = run_task2(raw_dataset_path)  # uses your Task 2.1 pipeline\n",
    "            t1 = time.perf_counter()\n",
    "            timings[c] = t1 - t0\n",
    "            print(f\"CPUs={c}: {timings[c]:.2f}s\")\n",
    "    return timings\n",
    "\n",
    "# Run and print speedups/efficiency\n",
    "_timings_2_2 = measure_task2_runtimes()\n",
    "print(\"\\nTimings:\", _timings_2_2)\n",
    "_base = _timings_2_2[1]\n",
    "for c in sorted(_timings_2_2):\n",
    "    _s = _base / _timings_2_2[c]\n",
    "    print(f\"CPUs={c}: speedup={_s:.2f} (ideal={c}, efficiency={_s/c:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564dd8d-e178-4b8a-b7ed-a92593bd0033",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4f89e2b36b4c13ad7dc538be95abeba",
     "grade": true,
     "grade_id": "cell-92afcb7261e6540b",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The speedup is not linear (ideal would be 4× at 4 CPUs). Using Amdahl’s-law style reasoning, the observed speedups imply ≈86–90% of the workload is parallelizable; the remaining serial fraction, plus scheduling/serialization and I/O overheads, limit scaling. \n",
    "\n",
    "In practice the following factors cause the sub-linear speedup:\n",
    "1. Non-parallel work (I/O, CSV parsing, small serial steps) — Amdahl’s law limits maximum speedup.\n",
    "2. Ray/Modin overheads (task scheduling, serialization/deserialization, actor startup).\n",
    "3. Data skew and partition imbalance (some partitions/partitions take longer → stragglers).\n",
    "4. Resource contention (disk bandwidth, memory pressure, GC) when adding workers.\n",
    "\n",
    "Practical takeaway: expect sub-linear but meaningful speedups; to improve scaling, profile the pipeline, minimize serialization and small tasks, tune partitioning, and avoid re-initializing Ray in the timed loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f11961a-905b-4c4f-bb46-8b7b862de72b",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "#### Task 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3993703c-e62f-46e8-91d1-ec170e91de0b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7facd796d86f07184d829f936175f454",
     "grade": false,
     "grade_id": "cell-464b5a0a19c47bb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 20:40:57,724\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plain merge sort algorithm \n",
    "\"\"\"\n",
    "import heapq\n",
    "from typing import List\n",
    "import time\n",
    "import ray\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "num_workers = 4\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=num_workers)\n",
    "\n",
    "def merge(sublists: List[list]) -> list:\n",
    "    \"\"\"\n",
    "    Merge sorted sublists into a single sorted list.\n",
    "\n",
    "    :param sublists: List of sorted lists\n",
    "    :return: Merged result\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    sublists = [sublist for sublist in sublists if len(sublist)> 0]\n",
    "    heap = [(sublist[0], i, 0) for i, sublist in enumerate(sublists)]\n",
    "    heapq.heapify(heap)\n",
    "    while len(heap):\n",
    "        val, i, list_ind = heapq.heappop(heap)\n",
    "        result.append(val)\n",
    "        if list_ind+1 < len(sublists[i]):\n",
    "            heapq.heappush(heap, (sublists[i][list_ind+1], i, list_ind+1))\n",
    "    return result\n",
    "\n",
    "def plain_merge_sort(collection: list, npartitions: int = 4) -> list:\n",
    "    \"\"\"\n",
    "    Sorts a list using the merge sort algorithm. Breaks the list into multiple partitions.\n",
    "\n",
    "    :param collection: A mutable ordered collection with comparable items.\n",
    "    :return: The same collection ordered in ascending order.\n",
    "\n",
    "    Time Complexity: O(n log n)\n",
    "\n",
    "    Examples:\n",
    "    >>> merge_sort([0, 5, 3, 2, 2])\n",
    "    [0, 2, 2, 3, 5]\n",
    "    >>> merge_sort([])\n",
    "    []\n",
    "    >>> merge_sort([-2, -5, -45])\n",
    "    [-45, -5, -2]\n",
    "\n",
    "    Modified from: https://github.com/TheAlgorithms/Python/\n",
    "    \"\"\"\n",
    "\n",
    "    if len(collection) < npartitions:\n",
    "        return sorted(collection)\n",
    "    breaks = [i*len(collection)//npartitions for i in range(npartitions)]\n",
    "    breaks.append(len(collection))\n",
    "    sublists = [collection[breaks[i]:breaks[i+1]] for i in range(len(breaks)-1)]\n",
    "    sorted_sublists = [plain_merge_sort(sublist, npartitions=2) for sublist in sublists] # just use 2 partitions in recursive calls\n",
    "    return merge(sorted_sublists)\n",
    "\n",
    "@ray.remote\n",
    "def ray_merge_sort(collection, pair, npartitions):\n",
    "    return plain_merge_sort(collection[pair[0]:pair[1]], npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5dcac7b-bb51-4f99-9773-9cc9e9af1f27",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f099b3075af9881b2ffe8138d263a43",
     "grade": false,
     "grade_id": "cell-9f98e590bf91b999",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def merge_sort_ray(collection_ref: ray.ObjectRef, length: int, npartitions: int = 4) -> list:\n",
    "    \"\"\"\n",
    "    Merge sort with ray\n",
    "    \"\"\"\n",
    "    ## DO NOT MODIFY: START    \n",
    "    breaks = [i*length//npartitions for i in range(npartitions)]\n",
    "    breaks.append(length)\n",
    "    # Keep track of partition end points\n",
    "    sublist_end_points = [(breaks[i], breaks[i+1]) for i in range(len(breaks)-1)]\n",
    "    ## DO NOT MODIFY: END\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Launch Ray tasks for each partition\n",
    "    futures = [\n",
    "        ray_merge_sort.remote(collection_ref, pair, npartitions)\n",
    "        for pair in sublist_end_points\n",
    "    ]\n",
    "    sorted_sublists = ray.get(futures)\n",
    "    # Pass your list of sorted sublists to `merge`\n",
    "    return merge(sorted_sublists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d062c9-985a-48cc-8c40-20960bb756ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bffc4f4b06298de874c802d247af3e0f",
     "grade": true,
     "grade_id": "cell-695d6f8d5e9a93e2",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain sorting: 18.880656480789185\n",
      "Ray sorting: 12.017321348190308\n",
      "Speedup:  1.5711202133770374\n"
     ]
    }
   ],
   "source": [
    "# We will be testing your code for a list of size 10M. Feel free to edit this for debugging. \n",
    "list1 = list(np.random.randint(low=0, high=1000, size=2_000_000))\n",
    "list2 = [c for c in list1] # make a copy\n",
    "length = len(list2)\n",
    "list2_ref = ray.put(list2) # insert into the driver's object store\n",
    "\n",
    "start1 = time.time()\n",
    "list1 = plain_merge_sort(list1, npartitions=num_workers)\n",
    "end1 = time.time()\n",
    "time_baseline = end1 - start1\n",
    "print(\"Plain sorting:\", time_baseline)\n",
    "\n",
    "start2 = time.time()\n",
    "list2 = merge_sort_ray(collection_ref=list2_ref, length=length, npartitions=num_workers)\n",
    "end2 = time.time()\n",
    "time_ray = end2 - start2\n",
    "print(\"Ray sorting:\", time_ray)\n",
    "\n",
    "speedup = time_baseline / time_ray\n",
    "print(\"Speedup: \", speedup)\n",
    "# Save timing metrics to JSON\n",
    "results = {\n",
    "    \"time_baseline\": time_baseline,\n",
    "    \"time_ray\": time_ray,\n",
    "    \"speedup\": speedup\n",
    "}\n",
    "assert sorted(list1) == list2, \"Sorted lists are not equal\"\n",
    "assert speedup > 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa5bbf-b2b1-41c1-8423-357aec5ff0aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "639b699e307ea5285f5e74ef1880b1db",
     "grade": false,
     "grade_id": "cell-0048da95939f323d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Task 3.2\n",
    "The ideal speedup you can get for a task with 4 workers is, of course, 4. Can you estimate the theoretical maximum speedup you can get for the above merge sort algorithm (in terms of the time for sorting sublists, and the time for merging)? How do you account for the difference between the theoretical result and the observed speedup with Ray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "062b5518-279f-4357-9290-3ccb8dded4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrumented timings: {'t_sort_sublists': 17.238127594813704, 't_merge': 0.6717044711112976, 't_total': 17.909832065925002}\n",
      "Theoretical S_max(p=4) ≈ 3.60x\n",
      "Observed Ray speedup S_obs = 1.57x; gap = 2.02x\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "\n",
    "def instrumented_four_way_mergesort(lst, npartitions: int = 4):\n",
    "    \"\"\"Time the sublist sorts (parallelizable) and the final merge (mostly serial).\"\"\"\n",
    "    n = len(lst)\n",
    "    breaks = [i * n // npartitions for i in range(npartitions)] + [n]\n",
    "    parts = [lst[breaks[i]:breaks[i + 1]] for i in range(npartitions)]\n",
    "\n",
    "    # Time sublist sorts (represents work that would be split across p workers)\n",
    "    t0 = time.perf_counter()\n",
    "    sorted_parts = [plain_merge_sort(p) for p in parts]\n",
    "    t1 = time.perf_counter()\n",
    "    t_sort_sublists = t1 - t0\n",
    "\n",
    "    # Time serial merge on the driver\n",
    "    t2 = time.perf_counter()\n",
    "    _merged = list(heapq.merge(*sorted_parts))\n",
    "    t3 = time.perf_counter()\n",
    "    t_merge = t3 - t2\n",
    "\n",
    "    return {\n",
    "        \"t_sort_sublists\": t_sort_sublists,\n",
    "        \"t_merge\": t_merge,\n",
    "        \"t_total\": (t1 - t0) + (t3 - t2),\n",
    "    }\n",
    "\n",
    "\n",
    "def theoretical_speedup_from_timings(tinfo: dict, p: int = 4):\n",
    "    \"\"\"S_max(p) = T_total / (T_sort/p + T_merge).\"\"\"\n",
    "    Tsort, Tmerge, Ttotal = tinfo[\"t_sort_sublists\"], tinfo[\"t_merge\"], tinfo[\"t_total\"]\n",
    "    return Ttotal / ((Tsort / p) + Tmerge)\n",
    "\n",
    "# Use the same list length as in the timing experiment above to keep apples-to-apples\n",
    "N_theory = length if 'length' in globals() else 200_000\n",
    "_arr = list(np.random.randint(low=0, high=10_000_000, size=N_theory))\n",
    "_tinfo = instrumented_four_way_mergesort(_arr, npartitions=4)\n",
    "_Smax = theoretical_speedup_from_timings(_tinfo, p=4)\n",
    "\n",
    "print(\"Instrumented timings:\", _tinfo)\n",
    "print(f\"Theoretical S_max(p=4) ≈ {_Smax:.2f}x\")\n",
    "if 'speedup' in globals():\n",
    "    print(f\"Observed Ray speedup S_obs = {speedup:.2f}x; gap = {_Smax - speedup:.2f}x\")\n",
    "else:\n",
    "    print(\"Run the Task 3.1 timing cell above to compute the observed Ray speedup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f6e91-5f4f-4a95-986b-5c68abdf40ca",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea54f668c59b2be4a96999da24d2428c",
     "grade": true,
     "grade_id": "cell-6362a0cee0945609",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Accounting for the difference to the observed speedup:**\n",
    "- The merge phase T_merge is serial in our implementation (final heap-merge on the driver), so it limits speedup by Amdahl’s law. Even with perfect parallel sorting, we cannot exceed 1 + T_sort/T_merge.\n",
    "- Practical overheads (scheduling, serialization, data movement, object copies, driver-side merge work, load imbalance, GC and memory pressure, I/O) further reduce observed speedup below the ideal S(p)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
